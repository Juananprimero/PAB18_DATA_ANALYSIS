{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tarea 8. Análisis de datos de indicadores de enfermedades crónicas\n",
    "Autores:   \n",
    "      Javier Moreno Venegas  \n",
    "      María del Rocío Cabello Toscano  \n",
    "      Juan Antonio Jiménez Guerrero  \n",
    "Fecha: 05/06/2018    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Esta tarea consiste en analizar los datos de indicadores de enfermedades crónicas que se proporcionan en el portal data.gov en la dirección https://catalog.data.gov/dataset/u-s-chronic-disease-indicators-cdi. El objetivo de la tarea es usar Spark para obtener, a partir del fichero csv que contiene los datos, una tabla similar a la tabla I del informe que se aporta en la página, que es el año 2013, que incluya dos columnas: Indicator Group e Individual Measures.\n",
    "La tarea se realizará en dos versiones: usando la API de RDDs y usando la de datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación exponemos el código del programa utilizando la API de RDDs. Para ello usamos el módulo pySpark que nos permite el análisis de gran cantidad de datos de manera concurrente. El filtro principal usado corresponde a los datos del año 2013 y escogemos las columnas correspondientes a Indicator Group e Individual Measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Alcohol', 9)\n",
      "('Arthritis', 10)\n",
      "('Asthma', 8)\n",
      "('Cancer', 3)\n",
      "('Cardiovascular Disease', 17)\n",
      "('Chronic Kidney Disease', 4)\n",
      "('Chronic Obstructive Pulmonary Disease', 14)\n",
      "('Diabetes', 17)\n",
      "('Disability', 1)\n",
      "('Immunization', 1)\n",
      "('Mental Health', 2)\n",
      "('Nutrition', 1)\n",
      "('Older Adults', 2)\n",
      "('Overarching Conditions', 13)\n",
      "('Reproductive Health', 1)\n",
      "('SID', 2)\n",
      "('Tobacco', 13)\n",
      "('UDS', 1)\n",
      "('USDA', 1)\n",
      "Computing time:  6.409118175506592\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import time\n",
    "\n",
    "def main(file_name: str) -> None:\n",
    "\n",
    "    spark_conf = SparkConf()\n",
    "    spark_context = SparkContext(conf=spark_conf)\n",
    "\n",
    "    logger = spark_context._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\").setLevel(logger.Level.WARN)\n",
    "\n",
    "    start_computing_time = time.time()\n",
    "\n",
    "    topics = \\\n",
    "    spark_context.textFile(file_name) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .filter(lambda list: list[0] == '2013') \\\n",
    "    .map(lambda list: (list[5].strip(\" \\\"\"), list[6].strip(\" \\\"\"))) \\\n",
    "    .filter(lambda list: list[0] != 'Topic' and list[1] != 'Question') \\\n",
    "    .distinct() \\\n",
    "    .map(lambda list: (list[0],1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .sortBy(lambda pair: pair[0]) \n",
    "\n",
    "    result = topics.collect()\n",
    "\n",
    "    for pair in result:\n",
    "        print(pair)\n",
    "\n",
    "    total_computing_time = time.time() - start_computing_time\n",
    "    print(\"Computing time: \", str(total_computing_time))\n",
    "\n",
    "    spark_context.stop()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Python program that uses Apache Spark to find \n",
    "    \"\"\"\n",
    "    main(\"datos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Repetimos el mismo procedimiento,pero esta vez con la API de dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               Topic|count|\n",
      "+--------------------+-----+\n",
      "|                 SID|    2|\n",
      "|                USDA|    1|\n",
      "|             Alcohol|    9|\n",
      "|           Arthritis|   10|\n",
      "|              Asthma|    8|\n",
      "|              Cancer|    3|\n",
      "|Cardiovascular Di...|   17|\n",
      "|Chronic Kidney Di...|    4|\n",
      "|Chronic Obstructi...|   14|\n",
      "|            Diabetes|   17|\n",
      "|          Disability|    1|\n",
      "|        Immunization|    1|\n",
      "|       Mental Health|    2|\n",
      "|Nutrition, Physic...|   25|\n",
      "|        Older Adults|    2|\n",
      "|         Oral Health|    1|\n",
      "|Overarching Condi...|   13|\n",
      "| Reproductive Health|    1|\n",
      "|             Tobacco|   13|\n",
      "+--------------------+-----+\n",
      "\n",
      "Computing time:  4.116872549057007\n"
     ]
    }
   ],
   "source": [
    "import time, sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def main(file_name) -> None:\n",
    "\n",
    "    spark_session = SparkSession \\\n",
    "        .builder \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    logger = spark_session._jvm.org.apache.log4j\n",
    "    logger.LogManager.getLogger(\"org\").setLevel(logger.Level.WARN)\n",
    "\n",
    "    start_computing_time = time.time()\n",
    "\n",
    "    data_frame = spark_session \\\n",
    "        .read \\\n",
    "        .format(\"csv\") \\\n",
    "        .options(header='true', inferschema='true') \\\n",
    "        .load(file_name)\n",
    "\n",
    "    #data_frame.printSchema()\n",
    "    #data_frame.show()\n",
    "\n",
    "    data_frame \\\n",
    "        .filter(data_frame[\"YearStart\"] == 2013) \\\n",
    "        .select(\"Topic\", \"Question\") \\\n",
    "        .distinct() \\\n",
    "        .orderBy(\"Topic\") \\\n",
    "        .groupBy(\"Topic\") \\\n",
    "        .count() \\\n",
    "        .show()\n",
    "\n",
    "    total_computing_time = time.time() - start_computing_time\n",
    "    print(\"Computing time: \", str(total_computing_time))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Python program that uses Apache Spark to read a .csv file and shows a table using the dataframe API.\n",
    "    \"\"\"\n",
    "\n",
    "    main(\"datos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar,los datos obtenidos son idénticos, sin embargo, el formato de salida es diferente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
